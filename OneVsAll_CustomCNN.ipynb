{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMCposT0+KoonSA7WIFu1Ju"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyCbnSQKb6xu","executionInfo":{"status":"ok","timestamp":1709505653435,"user_tz":-330,"elapsed":30513,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"}},"outputId":"6f375ad9-4443-412a-eaf1-a2b91427b228"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","cuda\n"]}],"source":["# In this notebook we will be using a Custom CNN model to train for only one class but with a GPU\n","\n","import torch\n","import torch.nn as nn\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","print(device)"]},{"cell_type":"code","source":["\"Let us first divide the test and train data into proper format\"\n","\n","import os\n","import shutil\n","import random\n","\n","base_dir = \"animals\"\n","\n","animal_dir = \"//content//drive//My Drive//DeepLearningCollab//Animal\"\n","os.makedirs(animal_dir , exist_ok=True)\n","\n","# Now we go into the directry containing the animals\n","\n","\n","source_dir = \"//content//drive//My Drive//DeepLearningCollab//animals\"\n","\n","#Destination Directories\n","\n","train_dir = os.path.join(animal_dir , 'train')\n","test_dir = os.path.join(animal_dir , 'test')\n","\n","\"If train and test directories do not exist\"\n","\n","os.makedirs(train_dir , exist_ok=True)\n","os.makedirs(test_dir , exist_ok=True)\n","\n","\"Now  we can list all the directories in source_dir\"\n","\n","animal_directories_list = os.listdir(source_dir)\n","\n","\"Now we will loop through the directories\"\n","\n","for animal in animal_directories_list:\n","    animal_D = os.path.join(source_dir , animal)\n","\n","    # Get a list of all the image files in the directory animal_D\n","    images = [i for i in os.listdir(animal_D) if os.path.isfile(os.path.join(animal_D , i))]\n","\n","    train_test_spllit = 0.8\n","\n","    num_train = int(len(images) * train_test_spllit)\n","\n","    random.shuffle(images) # Shuffle the images randomly\n","    train_images = images[:num_train]\n","\n","    test_images = images[num_train:]\n","\n","\n","    # Now we have already created test and train directory but here we have to also create test and train for each of the animals\n","\n","    train_dir_animal = os.path.join(train_dir , animal)\n","    test_dir_animal = os.path.join(test_dir , animal)\n","\n","    os.makedirs(train_dir_animal , exist_ok=True)\n","    os.makedirs(test_dir_animal , exist_ok=True)\n","\n","    \"Finally we have to move all the images from source directory to destination directory using shuttle\"\n","    for img in train_images:\n","        shutil.move(os.path.join(animal_D , img) , os.path.join(train_dir_animal , img))\n","    for img in test_images:\n","        shutil.move(os.path.join(animal_D , img) , os.path.join(test_dir_animal , img))"],"metadata":{"id":"AbJUckCwcEk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","\n","\n","train_path = os.path.join(animal_dir , \"train\")\n","test_path = os.path.join(animal_dir , \"test\")\n","\n","animal_dir = Path(animal_dir)\n","\n","\n","\n","# Using This we have also found out the class names and now we can convert them into Dict format\n","\n","from typing import Tuple , Dict , List\n","train_path = Path(train_path)\n","test_path = Path(test_path)\n","\n","def find_classes(directory : str) -> Tuple[List[str] , Dict[str,int]] :\n","\n","    \"Get the class_names first\"\n","\n","    classes = sorted(\n","        entry.name for entry in list(os.scandir(directory)) if entry.is_dir()\n","    )\n","\n","    # Get the case to handle any kind of error\n","\n","    if not classes:\n","        raise FileNotFoundError(f\"Could not find any classes in {directory}\")\n","\n","    classes_to_idx = {cls_name : i for i , cls_name in enumerate(classes)}\n","\n","    return classes , classes_to_idx"],"metadata":{"id":"BAAFdt2JcFa3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"Now we will enter into the part where we will be transforming the image and creating Datasets and DataLoaders\"\n","from torch.utils.data import DataLoader , TensorDataset , Dataset\n","from torchvision import transforms\n","import random\n","from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","\n","\"Lets write the transform for training Data and Testing Data\"\n","\n","transforms_train = transforms.Compose([\n","    transforms.Resize((224, 224)),   #must same as here\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(p = 0.5), # data augmentation\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n","])\n","transforms_test = transforms.Compose([\n","    transforms.Resize((224, 224)),   #must same as here\n","    transforms.CenterCrop((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","\"\"\"\n","Creating The Custom Dataset For One Vs Rest Classification:\n","\n","1) In the custom dataset we must take in decide which will be our target class and which will be the other classes.\n","2) The target class will have the label as 1 and the other classes will have label as 0\n","\n","\"\"\"\n","class CustomDataset(Dataset):\n","\n","    def __init__(self , target_class_index , target_directory : str , transform = None):\n","        super().__init__()\n","        self.label = []\n","        self.img = []\n","        self.transform = transform\n","\n","        # Here we can find the classes using find classes method\n","\n","        classes , _ = find_classes(target_directory)\n","        target_class_name = classes[target_class_index]\n","\n","        for Class in classes:\n","\n","            if(Class == target_class_name):\n","              limit = 1\n","            else:\n","              limit = 0.1\n","\n","            directory = os.path.join(target_directory , Class)\n","            for l , name in enumerate(os.listdir(directory)):\n","                if(l>=limit*len(os.listdir(directory))):\n","                    break\n","                final_path = os.path.join(directory , name)\n","                self.img.append(final_path)\n","\n","                if(Class == target_class_name):\n","                    self.label.append(1)\n","                else:\n","                    self.label.append(0)\n","\n","    def __len__(self):\n","        return len(self.img)\n","\n","    def load_image(self, index: int) -> Image.Image:\n","        image_path = self.img[index]\n","        img = Image.open(image_path)\n","        return img\n","\n","    def __getitem__(self, idx: int):\n","        image = Image.open(self.img[idx])\n","\n","\n","        Label = self.label[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, torch.tensor(Label, dtype=torch.long)"],"metadata":{"id":"HAZon3d3cIf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from sklearn.model_selection import KFold\n","\n","# Define your CustomDataset class and other necessary imports\n","# Assuming you have already defined CustomDataset and other imports\n","\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n","        super(Bottleneck, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n","\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n","\n","        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n","        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n","\n","        self.i_downsample = i_downsample\n","        self.stride = stride\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        identity = x.clone()\n","        x = self.relu(self.batch_norm1(self.conv1(x)))\n","\n","        x = self.relu(self.batch_norm2(self.conv2(x)))\n","\n","        x = self.conv3(x)\n","        x = self.batch_norm3(x)\n","\n","        #downsample if needed\n","        if self.i_downsample is not None:\n","            identity = self.i_downsample(identity)\n","        #add identity\n","        x+=identity\n","        x=self.relu(x)\n","\n","        return x\n","\n","class Block(nn.Module):\n","    expansion = 1\n","    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n","        super(Block, self).__init__()\n","\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n","        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n","        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n","\n","        self.i_downsample = i_downsample\n","        self.stride = stride\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","      identity = x.clone()\n","\n","      x = self.relu(self.batch_norm2(self.conv1(x)))\n","      x = self.batch_norm2(self.conv2(x))\n","\n","      if self.i_downsample is not None:\n","          identity = self.i_downsample(identity)\n","      print(x.shape)\n","      print(identity.shape)\n","      x += identity\n","      x = self.relu(x)\n","      return x\n","\n","\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n","        super(ResNet, self).__init__()\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.batch_norm1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU()\n","        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n","        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n","        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n","        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n","        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n","\n","    def forward(self, x):\n","        x = self.relu(self.batch_norm1(self.conv1(x)))\n","        x = self.max_pool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = x.reshape(x.shape[0], -1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n","        ii_downsample = None\n","        layers = []\n","\n","        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n","            ii_downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(planes*ResBlock.expansion)\n","            )\n","\n","        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n","        self.in_channels = planes*ResBlock.expansion\n","\n","        for i in range(blocks-1):\n","            layers.append(ResBlock(self.in_channels, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","def ResNet50(num_classes, channels=3):\n","    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n","\n","def ResNet101(num_classes, channels=3):\n","    return ResNet(Bottleneck, [3,4,23,3], num_classes, channels)\n","\n","def ResNet152(num_classes, channels=3):\n","    return ResNet(Bottleneck, [3,8,36,3], num_classes, channels)\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n"],"metadata":{"id":"AgcKMS5Nc7pv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import KFold, train_test_split\n","\n","class_list , _ = find_classes(train_path)\n","\n","# Define Stratified K-Fold cross-validation\n","skf = KFold(n_splits=3, shuffle=True)\n","\n","for i in range(90):\n","  print(class_list[i])\n","\n","  # Make the lists for the confusion Matrix\n","  true_labels = []\n","  predicted_labels = []\n","\n","  # Load Custom Training Dataset And Testing Dataset\n","\n","  train_dataset = CustomDataset(i , train_path , transform = transforms_train)\n","  test_dataset = CustomDataset(i , test_path , transform = transforms_test)\n","\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=24, shuffle=True, num_workers=2)\n","\n","  #### Train model\n","\n","  from sklearn.model_selection import KFold\n","\n","  train_loss=[]\n","  train_accuracy=[]\n","  test_loss=[]\n","  test_accuracy=[]\n","\n","\n","  # Extract indices\n","  indices = list(range(len(train_dataset)))\n","  # Define Stratified K-Fold cross-validation\n","  skf = KFold(n_splits=3, shuffle=True)\n","\n","  # Iterate over folds\n","  for fold, (train_index, val_index) in enumerate(skf.split(indices, [train_dataset.label[idx] for idx in indices])):\n","\n","      val_true_labels = []\n","      val_predicted_labels = []\n","\n","      print(\"FOLD : \" , fold)\n","      train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n","      val_sampler = torch.utils.data.SubsetRandomSampler(val_index)\n","\n","      # Create data loaders for training and validation\n","      train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=24, sampler=train_sampler, num_workers=4)\n","      val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=24, sampler=val_sampler, num_workers=4)\n","\n","      model = ResNet50(2)\n","\n","      # Since Resnet50 has 1000 out_features we will need to change it because our model has 1000 features.\n","\n","      num_features = model.fc.in_features\n","      # Add a fully-connected layer for classification\n","      model.fc = nn.Linear(num_features, 2)\n","      model = model.to(device)\n","\n","      criterion = nn.CrossEntropyLoss()\n","      optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n","\n","\n","\n","\n","      num_epochs = 25\n","      for epoch in range(num_epochs):\n","        epoch_true_labels = []\n","        epoch_predicted_labels = []\n","        print(\"Epoch {} running\".format(epoch)) #(printing message)\n","        model.train()\n","        running_loss = 0\n","        running_corrects = 0\n","        total_train = 0\n","\n","        # Now Load A Batch Of Images\n","\n","        for i , (inputs , labels) in enumerate(train_loader):\n","          inputs = inputs.to(device)\n","          labels = labels.to(device)\n","\n","          # Forward Inputs and Get Outputs\n","          optimizer.zero_grad()\n","          outputs = model.forward(inputs)\n","\n","          _ , preds = torch.max(outputs , 1)\n","\n","          loss = criterion(outputs , labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","          running_loss += loss.item()\n","          total_train += labels.size(0)\n","\n","          running_corrects += torch.sum(preds == labels.data).item()\n","\n","        epoch_loss = running_loss / len(train_sampler)\n","        epoch_acc = running_corrects / total_train *100\n","\n","        # Append Result\n","\n","        train_loss.append(epoch_loss)\n","        train_accuracy.append(epoch_acc)\n","\n","        # Print Progress\n","\n","        print('[Train #{}] Loss: {:.4f} Acc: {:.4f}%'.format(epoch+1, epoch_loss, epoch_acc))\n","\n","        # Testing Part\n","\n","        model.eval()\n","        with torch.no_grad():\n","          running_loss = 0\n","          running_corrects = 0\n","          total_val = 0\n","\n","          for i , (inputs , labels) in enumerate(val_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model.forward(inputs)\n","            _ , preds = torch.max(outputs , 1)\n","            loss = criterion(outputs , labels)\n","\n","            running_loss += loss.item()\n","            total_val += labels.size(0)\n","            running_corrects += torch.sum(preds == labels.data).item()\n","            val_true_labels.extend(labels.cpu().numpy())\n","            val_predicted_labels.extend(preds.cpu().numpy())\n","            epoch_true_labels.extend(labels.cpu().numpy())\n","            epoch_predicted_labels.extend(preds.cpu().numpy())\n","\n","\n","\n","          epoch_loss = running_loss / len(val_sampler)\n","          epoch_acc = running_corrects / total_val *100\n","\n","          # Append result\n","\n","          test_loss.append(epoch_loss)\n","          test_accuracy.append(epoch_acc)\n","\n","          # Print progress\n","          print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% '.format(epoch+1, epoch_loss, epoch_acc))\n","\n","          # Print Confusion Matrix After every epoch\n","\n","      print('Confusion Matrix on the validation set Fold :' , fold)\n","      print(confusion_matrix(val_true_labels, val_predicted_labels))\n","\n","  plt.figure(figsize=(10, 7))\n","  train_accuracy_tensor = torch.tensor(train_accuracy).cpu()\n","  test_accuracy_tensor = torch.tensor(test_accuracy).cpu()\n","\n","  plt.plot(train_accuracy_tensor, color='green', label='train accuracy')\n","  plt.plot(test_accuracy_tensor, color='blue', label='validation accuracy')\n","  plt.xlabel('epochs')\n","  plt.ylabel('accuracy')\n","  plt.legend()\n","  plt.show()\n","\n","  with torch.no_grad():\n","    running_corrects = 0\n","    total_test = 0\n","\n","    for i , (inputs , labels) in enumerate(test_loader):\n","      inputs = inputs.to(device)\n","      labels = labels.to(device)\n","      outputs = model.forward(inputs)\n","      _ , preds = torch.max(outputs , 1)\n","      running_corrects += torch.sum(preds == labels.data).item()\n","      total_test += labels.size(0)\n","      true_labels.extend(labels.cpu().numpy())\n","      predicted_labels.extend(preds.cpu().numpy())\n","\n","    epoch_acc = running_corrects / total_test *100\n","\n","    print(\"Test_Accuracy :\" , epoch_acc)\n","\n","    print('Confusion Matrix on the test set')\n","    print(confusion_matrix(true_labels, predicted_labels))\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"12_C60X0COtVPnqGK-bt0yL9Rp-ZSLIu_"},"id":"KZN6GOaxdGWB","executionInfo":{"status":"ok","timestamp":1709551230341,"user_tz":-330,"elapsed":14433107,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"}},"outputId":"3eebf363-dd9c-411c-afa3-c9394306ee1a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"lZJnt_ZqeiJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZBZdSIMuIbN7"},"execution_count":null,"outputs":[]}]}